
# Chap07.2 - í…ì„œí”Œë¡œ ì¶”ìƒí™”ì™€ ê°„ì†Œí™”, TFLearn

> **TFLearn**ì€ [Chap07.1 Estimator](http://excelsior-cjh.tistory.com/157)ì—ì„œ ì‚´í´ë³¸ `tf.estimator`ì™€ ë§ˆì°¬ê°€ì§€ë¡œ í…ì„œí”Œë¡œì˜ ì¶”ìƒí™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. ì´ë²ˆì—ëŠ” TFLearnì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ì.

 

## 7.3 TFLearn

### 7.3.1 ì„¤ì¹˜

[**TFLearn**](http://tflearn.org/)ì€ í…ì„œí”Œë¡œì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ë³„ë„ì˜ ì„¤ì¹˜ê°€ í•„ìš”í•˜ë‹¤. Terminal(ë˜ëŠ” cmdì°½)ì— `pip` ëª…ë ¹ì„ ì´ìš©í•´ ì„¤ì¹˜í•  ìˆ˜ ìˆë‹¤.

```bash
pip install tflearn
```

 

### 7.3.2 CNN 

TFLearnì€ [Chap07.1 - tf.estimator](http://excelsior-cjh.tistory.com/157)ì™€ ìœ ì‚¬í•˜ì§€ë§Œ, TFLearnì„ ì‚¬ìš©í•˜ë©´ ì¡°ê¸ˆ ë” ê¹”ë”í•˜ê²Œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. [TFLearn.org](http://tflearn.org/)ì—ì„œëŠ” TFLearnì„ ë‹¤ìŒê³¼ ê°™ì´ ì†Œê°œí•˜ê³  ìˆë‹¤.

> - Easy-to-use and understand high-level API for implementing deep neural networks, with tutorial and examples.
- Fast prototyping through highly modular built-in neural network layers, regularizers, optimizers, metrics...
- Full transparency over Tensorflow. All functions are built over tensors and can be used independently of TFLearn.
- Powerful helper functions to train any TensorFlow graph, with support of multiple inputs, outputs and optimizers.
- Easy and beautiful graph visualization, with details about weights, gradients, activations and more...
- Effortless device placement for using multiple CPU/GPU.

 

TFLearnì—ì„œì˜ ëª¨ë¸ ìƒì„±ì€ `regression()`ì„ ì‚¬ìš©í•˜ì—¬ ë˜í•‘ë˜ê³  ë§ˆë¬´ë¦¬ëœë‹¤. `regression()`í•¨ìˆ˜ì—ì„œ ì†ì‹¤í•¨ìˆ˜(`loss`) ë° ìµœì í™”(`optimizer`)ë¥¼ ì„¤ì •í•´ì¤€ë‹¤.

ê·¸ë ‡ë‹¤ë©´, TFLearnì„ ì´ìš©í•´ MNIST ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” CNN ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ë„ë¡ í•˜ì.


```python
import numpy as np
import tflearn
import tflearn.datasets.mnist as mnist
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.normalization import local_response_normalization
from tflearn.layers.estimator import regression

# ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê³  ê¸°ë³¸ì ì¸ ë³€í™˜ì„ ìˆ˜í–‰
train_x, train_y, test_x, test_y = mnist.load_data(one_hot=True, 
                                                   data_dir='../data')
train_x = train_x.reshape([-1, 28, 28, 1])
test_x = test_x.reshape([-1, 28, 28, 1])

# Building the network
CNN = input_data(shape=[None, 28, 28, 1], name='input')
CNN = conv_2d(CNN, 32, 5, activation='relu', regularizer="L2")
CNN = max_pool_2d(CNN, 2)
CNN = local_response_normalization(CNN)
CNN = conv_2d(CNN, 64, 5, activation='relu', regularizer='L2')
CNN = max_pool_2d(CNN, 2)
CNN = local_response_normalization(CNN)
CNN = fully_connected(CNN, 1024, activation=None)
CNN = dropout(CNN, 0.5)
CNN = fully_connected(CNN, 10, activation='softmax')
CNN = regression(CNN, optimizer='adam', learning_rate=0.0001, 
                 loss='categorical_crossentropy', name='target')

# Training the network
model = tflearn.DNN(CNN, tensorboard_verbose=0, 
                    tensorboard_dir='./MNIST_tflearn_board/', 
                    checkpoint_path='./MNIST_tflearn_checkpoints/checkpoint')
model.fit({'input': train_x}, {'target': train_y}, n_epoch=3,
          validation_set=({'input': test_x}, {'target': test_y}),
          snapshot_step=1000, show_metric=True, run_id='convnet_mnist')
```

    Training Step: 2579  | total loss: [1m[32m0.10814[0m[0m | time: 5.414s
    | Adam | epoch: 003 | loss: 0.10814 - acc: 0.9841 -- iter: 54976/55000
    Training Step: 2580  | total loss: [1m[32m0.10413[0m[0m | time: 6.558s
    | Adam | epoch: 003 | loss: 0.10413 - acc: 0.9826 | val_loss: 0.04778 - val_acc: 0.9861 -- iter: 55000/55000
    --
    INFO:tensorflow:/D/dev/LearningTensorFlow/Chap07-TF_Abstractions/MNIST_tflearn_checkpoints/checkpoint-2580 is not in all_model_checkpoint_paths. Manually adding it.
    INFO:tensorflow:/D/dev/LearningTensorFlow/Chap07-TF_Abstractions/MNIST_tflearn_checkpoints/checkpoint-2580 is not in all_model_checkpoint_paths. Manually adding it.


 

ìœ„ì˜ ì½”ë“œì—ì„œ `tflearn.DNN()`í•¨ìˆ˜ëŠ” `tf.estimator.Estimator()`ì™€ ë¹„ìŠ·í•œ ê¸°ëŠ¥ì„ í•˜ëŠ”ë°, `regression()`ìœ¼ë¡œ ë˜í•‘ëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ ì „ë‹¬í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ë˜í•œ í…ì„œë³´ë“œ(TensorBoard)ì™€ ì²´í¬í¬ì¸íŠ¸(checkpoint) ë””ë ‰í„°ë¦¬ ë“±ì„ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ëª¨ë¸ ì í•© ì—°ì‚°ì€ `.fit()` ë©”ì„œë“œë¥¼ ì´ìš©í•´ ìˆ˜í–‰ëœë‹¤. 

ëª¨ë¸ ì í•©(`.fit()`), ì¦‰ í•™ìŠµì´ ì™„ë£Œë˜ë©´, ë‹¤ìŒê³¼ ê°™ì€ ë©”ì†Œë“œë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ í‰ê°€, ì˜ˆì¸¡, ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸° ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

| ë©”ì„œë“œ                         | ì„¤ëª…                                               |
| ------------------------------ | -------------------------------------------------- |
| `evaluate(X, Y, batch_size)`   | ì£¼ì–´ì§„ ìƒ˜í”Œì—ì„œ ëª¨ë¸ì„ í‰ê°€                        |
| `fit(X, Y, n_epoch)`           | ì…ë ¥ feature `X`ì™€ íƒ€ê²Ÿ `Y`ë¥¼ ëª¨ë¸ì— ì ìš©í•˜ì—¬ í•™ìŠµ |
| `get_weights(weight_tensor)`   | ë³€ìˆ˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜                               |
| `load(model_file)`             | í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸°                      |
| `predict(x)`                   | ì£¼ì–´ì§„ `x` ë°ì´í„°ë¥¼ ëª¨ë¸ì„ ì´ìš©í•´ ì˜ˆì¸¡             |
| `save(model_file)`             | í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥                          |
| `set_weights(tensor, weights)` | ì£¼ì–´ì§„ ê°’ì„ í…ì„œ ë³€ìˆ˜ì— í• ë‹¹                       |


```python
# Evaluate the network
evaluation = model.evaluate(X=test_x, Y=test_y, batch_size=128)
print(evaluation)

# Predict
pred = model.predict(test_x)
accuarcy = (np.argmax(pred, 1)==np.argmax(test_y, 1)).mean()
print(accuarcy)
```

    [0.9861]
    0.9861


 

### 7.3.3. RNN

ì´ë²ˆì—ëŠ” TFLearnì„ ì´ìš©í•´ RNNì„ êµ¬í˜„í•´ ë³´ë„ë¡í•˜ì. êµ¬í˜„í•  RNN ëª¨ë¸ì€ ì˜í™” ë¦¬ë·°ì— ëŒ€í•œ ê°ì„±ë¶„ì„ìœ¼ë¡œ, ë¦¬ë·°ì— ëŒ€í•´ ì¢‹ê±°ë‚˜/ë‚˜ì˜ê±°ë‚˜ ë‘ ê°œì˜ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ë°ì´í„°ëŠ” í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ê°ê° 25,000ê°œë¡œ ì´ë£¨ì–´ì§„ [IMDb](https://www.imdb.com/interfaces/) ë¦¬ë·° ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤.


```python
import tflearn
from tflearn.data_utils import to_categorical, pad_sequences
from tflearn.datasets import imdb

# IMDb ë°ì´í„°ì…‹ ë¡œë“œ
(train_x, train_y), (test_x, test_y), _ = imdb.load_data(path='../data/imdb.pkl', 
                                                         n_words=10000,
                                                         valid_portion=0.1)
```

 

ìœ„ì—ì„œ ë¶ˆëŸ¬ì˜¨ IMDb ë°ì´í„°ëŠ” ê°ê° ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ 100ìœ¼ë¡œ í•˜ì—¬ `tflearn.data_utils.pad_sequences()`ë¥¼ ì‚¬ìš©í•´ ì œë¡œ íŒ¨ë”©ìœ¼ë¡œ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì¤€ë‹¤.


```python
# Sequence padding and Converting labels to binary vectors
train_x = pad_sequences(train_x, maxlen=100, value=0.)
test_x = pad_sequences(test_x, maxlen=100, value=0.)
train_y = to_categorical(train_y, nb_classes=2)
test_y = to_categorical(test_y, nb_classes=2)
```

 

ê·¸ëŸ°ë‹¤ìŒ, `tflearn.embedding()`ìœ¼ë¡œ ë²¡í„° ê³µê°„ìœ¼ë¡œì˜ ì„ë² ë”©ì„ ìˆ˜í–‰í•œë‹¤. ì•„ë˜ì˜ ì½”ë“œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´ ê° ë‹¨ì–´ëŠ” 128 í¬ê¸°ì¸ ë²¡í„°ì— ë§¤í•‘ëœë‹¤. ì´ë ‡ê²Œ ì„ë² ë”©ëœ ê²°ê³¼ë¥¼ `LSTM` layerì™€ `fully_connected` layerë¥¼ ì¶”ê°€í•´ ëª¨ë¸ì„ êµ¬ì„±í•´ì¤€ë‹¤.


```python
# Building a LSTM network
# Embedding
RNN = tflearn.input_data([None, 100])
RNN = tflearn.embedding(RNN, input_dim=10000, output_dim=128)

# LSTM Cell
RNN = tflearn.lstm(RNN, 128, dropout=0.8)
RNN = tflearn.fully_connected(RNN, 2, activation='softmax')
RNN = tflearn.regression(RNN, optimizer='adam', 
                         learning_rate=0.001, loss='categorical_crossentropy')

# Training the network
model = tflearn.DNN(RNN, tensorboard_verbose=0, 
                    tensorboard_dir='./IMDb-tflearn_board/')
model.fit(train_x, train_y, 
          validation_set=(test_x, test_y), 
          show_metric=True, batch_size=32)
```

    Training Step: 7039  | total loss: [1m[32m0.05996[0m[0m | time: 22.310s
    | Adam | epoch: 010 | loss: 0.05996 - acc: 0.9827 -- iter: 22496/22500
    Training Step: 7040  | total loss: [1m[32m0.05474[0m[0m | time: 23.425s
    | Adam | epoch: 010 | loss: 0.05474 - acc: 0.9845 | val_loss: 0.85953 - val_acc: 0.8064 -- iter: 22500/22500
    --



```python
# evaluate the network
evaluation = model.evaluate(test_x, test_y, batch_size=128)
print(evaluation)
```

    [0.8063999997138978]


 

ì•„ë˜ì˜ ì½”ë“œëŠ” ìœ„ì˜ ì½”ë“œë¥¼ í•©ì¹œ ì „ì²´ ì½”ë“œì´ë‹¤.


```python
import tflearn
from tflearn.data_utils import to_categorical, pad_sequences
from tflearn.datasets import imdb

# IMDb ë°ì´í„°ì…‹ ë¡œë“œ
(train_x, train_y), (test_x, test_y), _ = imdb.load_data(path='../data/imdb.pkl', 
                                                         n_words=10000,
                                                         valid_portion=0.1)


# Sequence padding and Converting labels to binary vectors
train_x = pad_sequences(train_x, maxlen=100, value=0.)
test_x = pad_sequences(test_x, maxlen=100, value=0.)
train_y = to_categorical(train_y, nb_classes=2)
test_y = to_categorical(test_y, nb_classes=2)

# Building a LSTM network
# Embedding
RNN = tflearn.input_data([None, 100])
RNN = tflearn.embedding(RNN, input_dim=10000, output_dim=128)

# LSTM Cell
RNN = tflearn.lstm(RNN, 128, dropout=0.8)
RNN = tflearn.fully_connected(RNN, 2, activation='softmax')
RNN = tflearn.regression(RNN, optimizer='adam', 
                         learning_rate=0.001, loss='categorical_crossentropy')

# Training the network
model = tflearn.DNN(RNN, tensorboard_verbose=0, 
                    tensorboard_dir='./IMDb-tflearn_board/')
model.fit(train_x, train_y, 
          validation_set=(test_x, test_y), 
          show_metric=True, batch_size=32)

# evaluate the network
evaluation = model.evaluate(test_x, test_y, batch_size=128)
print(evaluation)
```

 

### 7.3.4 ì •ë¦¬

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” TFLearnì„ ì‚´í´ë³´ê³ , ì´ë¥¼ ì´ìš©í•´ CNNê³¼ RNNì„ êµ¬í˜„í•´ ë³´ì•˜ë‹¤. ì´ ì™¸ì—ë„ TFLearnì— ëŒ€í•œ ì‚¬ìš©ë²• ë° ì˜ˆì œëŠ” http://tflearn.org/ ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
